Web Audio Processing: Use Cases and Requirementstable {border-collapse: collapse; border: 1px solid #000; font: normal 80%/140% arial, helvetica, sans-serif; color: #555; background: #fff;}td, th {border: 1px dotted #bbb; padding:.5em 1em; font-size: x-small; width: 10em; }caption {padding: 0 0 .5em 0; text-align: left; font-weight: 500; text-align: center; color: #666; background: transparent;}table a {padding: 1px; text-decoration: none; font-weight: bold; background: transparent;}table a:link {border-bottom: 1px dashed #ddd; color: #000;}table a:visited {border-bottom: 1px dashed #ccc; text-decoration: line-through; color: #808080;}table a:hover {border-bottom: 1px dashed #bbb; color: #666;}thead th, tfoot th {white-space: nowrap; border: 1px solid #000; text-align: center; color: black; background: #ddd;}tfoot td {border: 2px solid #000;}tbody { height: 300px; overflow: auto; }tbody th {color: #060606; }tbody th, tbody td {vertical-align: middle; text-align: center; }/* --- INLINES --- */em.rfc2119 {text-transform: lowercase;font-variant: small-caps;font-style: normal;color: #900;}h1 acronym, h2 acronym, h3 acronym, h4 acronym, h5 acronym, h6 acronym, a acronym,h1 abbr, h2 abbr, h3 abbr, h4 abbr, h5 abbr, h6 abbr, a abbr {border: none;}dfn {font-weight: bold;}a.internalDFN {color: inherit;border-bottom: 1px solid #99c;text-decoration: none;}a.externalDFN {color: inherit;border-bottom: 1px dotted #ccc;text-decoration: none;}a.bibref {text-decoration: none;}cite .bibref {font-style: normal;}code {color: #ff4500;}/* --- --- */ol.algorithm { counter-reset:numsection; list-style-type: none; }ol.algorithm li { margin: 0.5em 0; }ol.algorithm li:before { font-weight: bold; counter-increment: numsection; content: counters(numsection, ".") ") "; }/* --- TOC --- */.toc a, .tof a {text-decoration: none;}a .secno, a .figno {color: #000;}ul.tof, ol.tof {list-style: none outside none;}.caption {margin-top: 0.5em;font-style: italic;}/* --- TABLE --- */table.simple {border-spacing: 0;border-collapse: collapse;border-bottom: 3px solid #005a9c;}.simple th {background: #005a9c;color: #fff;padding: 3px 5px;text-align: left;}.simple th[scope="row"] {background: inherit;color: inherit;border-top: 1px solid #ddd;}.simple td {padding: 3px 10px;border-top: 1px solid #ddd;}.simple tr:nth-child(even) {background: #f0f6ff;}/* --- DL --- */.section dd > p:first-child {margin-top: 0;}.section dd > p:last-child {margin-bottom: 0;}.section dd {margin-bottom: 1em;}.section dl.attrs dd, .section dl.eldef dd {margin-bottom: 0;}height="48" width="72">Web Audio Processing: Use Cases andRequirementsW3CWorking Group Note 29 January 2013This version:http://www.w3.org/TR/2013/NOTE-webaudio-usecases-20130129/Latest published version:http://www.w3.org/TR/webaudio-usecases/Previous version:http://www.w3.org/TR/2012/WD-webaudio-usecases-20121004/Latest editor's draft:https://dvcs.w3.org/hg/audio/raw-file/tip/reqs/Overview.htmlEditors:Joe Berkovitz, NoteflightOlivier Thereaux, BritishBroadcasting Corporation (BBC)CopyrightÂ© 2013 W3CÂ®(MIT,ERCIM,Keio), All Rights Reserved.title="World Wide Web Consortium">W3C liability,trademarkand documentuse rules apply.AbstractThis document introduces a series of scenarios and a list ofrequirements guiding the work of the W3CAudio Working Group in its development of a web API for processing andsynthesis of audio on the web.Status of This DocumentThis section describes the status of this document at the time ofits publication. Other documents may supersede this document. A listof current W3Cpublications and the latest revision of this technical report can befound in the W3Ctechnical reports index at http://www.w3.org/TR/.This document was published by the AudioWorking Group as a Working Group Note. If you wish to make commentsregarding this document, please send them to public-audio@w3.org(subscribe,archives).All comments are welcome.Publication as a Working Group Note does not imply endorsement by the W3C Membership. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite this document as other than work in progress.This document was produced by a group operating under the 5February 2004 W3CPatent Policy. W3Cmaintains a publiclist of any patent disclosures made in connection with thedeliverables of the group; that page also includes instructions fordisclosing a patent. An individual who has actual knowledge of a patentwhich the individual believes contains EssentialClaim(s) must disclose the information in accordance with section6 of the W3C PatentPolicy.Table of Contents1.Introductionclass="secno">2. Web Audio Scenariosclass="secno">2.1 Video Chat Applicationclass="secno">2.2 3D game with music and convincingsound effectsclass="secno">2.3 Online music production toolclass="secno">2.4 Online radio broadcastclass="secno">2.5 Music Creation Environment withSampled Instrumentsclass="secno">2.6 Connected DJboothclass="secno">2.7 Playful sonification of userinterfacesclass="secno">2.8 Podcast on a flightclass="secno">2.9 Short film with director'scommentary and audio descriptionclass="secno">2.10 Web-based guitar practice serviceclass="secno">2.11 User Control of AudioA.Acknowledgements1. IntroductionWhat should the future web sound like? That was, in essence, themission of the W3C AudioWorking Group when it was chartered in early 2011 to "support thefeatures required by advanced interactive applications including theability to process and synthesize audio". Bringing audio processing andsynthesis capabilities to the Open Web Platform should allow developersto re-create well-loved audio software on the open web and add greatsound to web games and applications; it may also enable web developersto reinvent the world of audio and music by making it more connected,linked and social.This document attempts to describe the scenarios considered by thetitle="World Wide Web Consortium">W3C Audio Working Group inits work to define Web Audio technologies. Not intended to be acomprehensive list of things which the Web Audio standards will makepossible, it nevertheless attempts to:document a number of key applications of audio which Web Audiostandards should enable,provide a basis for discussion on the desired architecture of WebAudio standards,offer examples for early uses of the technology, which can then beused to gather feedback on the draft standard, andextract technical and architectural requirements for the Web AudioAPIs or libraries built upon it. The Notes and ImplementationConsiderations sections will note which constructs of thetitle="Web Audio API">Web Audio API Working Draft apply.Whenever possible, the document will note which features are yet to beimplemented or documented in that specification as oftitle="Web Audio API">02 August 2012.2. Web Audio ScenariosThis section will introduce a number of scenarios involving the use ofWeb Audio processing or synthesis technologies, and discussimplementation and architectural considerations.2.1 Video Chat ApplicationThree people have joined a three-way conversation through a webapplication. Each of them see the other two participants in splitwindows, and hear their voice in sync with the video.The application provides a simple interface to control the incomingaudio and video of the other participants: at any time, the user canmute the incoming streams, control the overall sound volume, or mutethemselves while continuing to send a live video stream through theapplication.Advanced controls are also available. In the "Audio" option panel,the user has the ability to adapt the incoming sound to their tastethrough a graphic equalizer interface, as well as a number of filtersfor voice enhancement, a feature which can be useful between peoplewith hearing difficulties, in imperfect listening environments, or tocompensate for poor transmission environments.Another option allows the user to change the spatialization of thevoices of their interlocutors; the default is a binaural mix matchingthe disposition of split-windows on the screen, but the interfacemakes it possible to reverse the left-right balance, or make the otherparticipants appear closer or further apart.The makers of the chat applications also offer a "fun" version whichallows users to distort (pitch, speed, other effects) their voice.They are considering adding the option to the default software, assuch a feature could also be used to protect one participants' privacyin some contexts.Notes andImplementation ConsiderationsThe processing capabilities needed by this scenario include:Mixing and spatialization of several sound sourcesControlling the gain (mute and volume control) ofseveral audio sourcesFiltering (EQ, voiceenhancement)Modifying the pitch and speed of sound sourcesThis scenario is also a good example of the need for audiocapture (from line in, internal microphone or other inputs). Weexpect this to be provided bytitle="HTML Media Capture">HTML Media Capture.The firstscenario in WebRTC's Use Cases and Requirements document hasbeen a strong inspiration for this scenario. Most of thetechnology, described above should be covered by thetitle="WebRTC 1.0: Real-time Communication Between Browsers">WebReal-Time Communication API. The scenario illustrates,however, the need to integrate audio processing with the handlingof RTC streams, with a technical requirement for processing of theaudio signal at both ends (capture of the user's voice and outputof its correspondents' conversation).Speed changes are currently unsupported by the Web Audio API.2.2 3D game with music and convincingsound effectsA commuter is playing a 3D first-person adventure game on theirmobile device. The game is built entirely using open web technologies,and includes rich, convincing sound piped through the commuter'sstereo headphones.As soon as the game starts, a musical background starts, loopsseamlessly, and transitions smoothly from one music track to anotheras the player enters a house. Some of the music is generated live, andreacts to the state of the game: tempo, time signature, noteproperties and envelopes change depending on the the health level ofthe characters and their actions.While walking in a corridor, the player can hear the muffled sound ofa ticking grandfather's clock. Following the direction of the soundand entering a large hall, the sound of the clock becomes clear,reverberating in the large hall. At any time, the sound of the clockspatialized in real-time based on the position of the player'scharacter in the room (relative to the clock) and the current cameraangle.The soundscape changes, bringing a more somber, scary atmosphere tothe scene: the once full orchestral underscore is slowly reduced,instrument by instrument, to a lonely and echoing cello. The playerequips a firearm. Suddenly, a giant snake springs from behind acorner, its hissing becoming a little louder as the snake turns itshead towards the player. The weapon fires at the touch of a key, andthe player can hear the sound of bullets in near-perfectsynchronization with the firing, as well as the sound of bulletsricocheting against walls. The sounds are played immediately after theplayer presses the key, but the action and video frame rate can remainsmooth even when a lot of sounds (bullets being fired, echoing andricocheting, sound of the impacts, etc) are played at the same time.The snake is now dead, and many flies gather around it, and around theplayer's character, buzzing and zooming in the virtual space of theroom.Notes andImplementation ConsiderationsDeveloping the soundscape for a game as the one described abovecan benefit from a modular, node based approach to audioprocessing. In our scenario, some of the processing needs tohappen for a number of sources at the same time (e.g room effects)while others (e.g mixing and spatialization) need to happen on aper-source basis. A graph-based API makes it very easy toenvision, construct and control the necessary processingarchitecture, in ways that would be possible with other kinds ofAPIs, but more difficult to implement. The fundamental AudioNodeconstruct in the Web Audio API supports this approach.While a single looping music background can be created today withthetitle="4.8.7 The audio element Ã¢â‚¬â€? HTML5">HTML5 <audio>element, the ability to transition smoothly from one musicalbackground to another requires additional capabilities that arefound in the Web Audio API including sample-accurate playbackscheduling and automated cross-fading of multiplesources. Related API features include AudioBufferSourceNode.start()and AudioParam.setValueAtTime().The musical background of the game not only involves seamlesslooping and transitioning of full tracks, but also the automatedcreation of generative music from basic building blocks oralgorithms ("Some of the music is generated live, and reacts tothe state of the game"), as well as the creation and evolution ofa musical score from multiple instrument tracks ("the once fullorchestral underscore is slowly reduced, instrument byinstrument"). Related requirements for such features are developedin details within the Onlinemusic production tool and MusicCreation Environment with Sampled Instruments scenarios.The scenario illustrates many aspects of the creation of acredible soundscape. The game character is evolving in a virtualthree-dimensional environment and the soundscape is at all timesspatialized: a panning model can be used to spatializesound sources in the game (AudioPanningNode); obstruction/ occlusion modeling is used to muffle the sound of theclock going through walls, and the sound of flies buzzing aroundwould need Doppler Shift simulation to sound believable(also supported by AudioPanningNode). The listener'sposition is part of this 3D model as well (AudioListener).As the soundscape changes from small room to large hall, the gamebenefits from the simulation of acoustic spaces,possibly through the use of a convolution engine forhigh quality room effects as supported by ConvolverNodein the Web Audio API.Many sounds in the scenario are triggered by events in the game,and would need to be played with low latency. The sound of thebullets as they are fired and ricochet against the walls, inparticular, illustrate a requirement for basic polyphonyand high-performance playback and processing of many sounds.These are supported by the general ability of the Web Audio API toinclude many sound-generating nodes with independent schedulingand high-throughput native algorithms.2.3 Online music production toolA music enthusiast creates a musical composition from audio mediaclips using a web-based Digital Audio Workstation (DAW) application.Audio "clips" are arranged on a timeline representing multiple tracksof audio. Each track's volume, panning, and effects may be controlledseparately. Individual tracks may be muted or soloed to previewvarious combination of tracks at a given moment. Audio effects may beapplied per-track as inline (insert) effects. Additionally, each trackcan send its signal to one or more global send effects which areshared across tracks. Sub-mixes of various combinations of tracks canbe made, and a final mix bus controls the overall volume of the mix,and may have additional insert effects.Insert and send effects include dynamics compressors (includingmulti-band), extremely high-quality reverberation, filters such asparametric, low-shelf, high-shelf, graphic EQ,etc. Also included are various kinds of delay effects such asping-pong delays, and BPM-synchronizeddelays with feedback. Various kinds of time-modulated effects areavailable such as chorus, phasor, resonant filter sweeps, and BPM-synchronizedpanners. Distortion effects include subtle tube simulators, andaggressive bit decimators. Each effect has its own UIfor adjusting its parameters. Real-time changes to the parameters canbe made (e.g. with a mouse) and the audible results heard with noperceptible lag.Audio clips may be arranged on the timeline with a high-degree ofprecision (with sample accurate playback). Certain clips may berepeated loops containing beat-based musical material, and aresynchronized with other such looped clips according to a certainmusical tempo. These, in turn, can be synchronized with sequencescontrolling real-time synthesized playback. The values of volume,panning, send levels, and each parameter of each effect can be changedover time, displayed and controlled through a powerful UIdealing with automation curves. These curves may be arbitrary and canbe used, for example, to control volume fade-ins, filter sweeps, andmay be synchronized in time with the music (beat synchronized).Visualizers may be applied for technical analysis of the signal.These visualizers can be as simple as displaying the signal level in aVU meter, or more complex such asreal-time frequency analysis, or L/Rphase displays.The actual audio clips to be arranged on the timeline are managed ina library of available clips. These can be searched and sorted in avariety of ways and with high-efficiency. Although the clips can becloud-based, local caching offers nearly instantaneous access andglitch-free playback.The final mix may be rendered at faster than real-time and thenuploaded and shared with others. The session representing the clips,timeline, effects, automation, etc. may also be shared with others forshared-mixing collaboration.Notes andImplementation ConsiderationsThis scenario details the large number of feature requirementstypically expected of professional audio software or hardware. Itencompasses many advanced audio control capabilities such asfiltering, effects, dynamics compression and control of variousaudio parameters.Building such an application may only be reasonably possible ifthe technology enables the control of audio with acceptableperformance, in particular for real-time processing andcontrol of audio parameters and sample accurate scheduling ofsound playback. Because performance is such a key aspect ofthis scenario, it should probably be possible to control thebuffer size of the underlying Audio API: this would allow userswith slower machines to pick a larger buffer setting that does notcause clicks and pops in the audio stream.The ability to visualize the samples and their processingbenefits from real-time time-domain and frequency analysis,as supplied by the Web Audio API's RealtimeAnalyzerNode.Clips must be able to be loaded into memory for fast playback.The Web Audio API's AudioBuffer and AudioBufferSourceNodeinterfaces address this requirement.Some sound sources may be purely algorithmic in nature, such asoscillators or noise generators. This implies the ability togenerate sound from both precomputed and dynamically computedarbitrary sound samples. The Web Audio API's ability to create anAudioBuffer from arrays of numerical samples, coupledwith the ability of JavaScriptAudioNode to supplynumerical samples on the fly, both address this requirement.The ability to schedule both audio clip playback and effectsparameter value changes in advance is essential to supportautomated mixdownTo export an audio file, the audio rendering pipeline must beable to yield buffers of sample frames directly, rather than beingforced to an audio device destination. Built-in codecs totranslate these buffers to standard audio file output formats arealso desirable.Typical per-channel effects such as panning, gain control,compression and filtering must be readily available in a native,high-performance implementation.Typical master bus effects such as room reverb must be readilyavailable. Such effects are applied to the entire mix as a finalprocessing stage. A single ConvolverNode is capableof simulating a wide range of room acoustics.2.4 Online radio broadcastA web-based online radio application supports one-to-many audiobroadcasting on various channels. For any one broadcast channel itexposes three separate user interfaces on different pages. Oneinterface is used by the broadcaster controlling a radio show on thechannel. A second interface allows invited guests to supply live audioto the show. The third interface is for the live online audiencelistening to the channel.The broadcaster interface supports live and recorded audio sourceselection as well as mixing of those sources. Audio sources include:any local microphoneprerecorded audio such as jingles or tracks from music librariesa remote microphone for a remote guestA simple mixer lets the broadcaster control the volume, pan andeffects processing for each local or remote audio source, blendingthem into a single stereo output mix that is broadcast as the show'scontent. Indicators track the level of each active source. This mixeralso incorporates some automatic features to make the broadcaster'slife easier, including ducking of prerecorded audio sources when anylocal or remote microphone source is active. Muting (un-muting) ofsources causes an automatic fast volume fade-out(in) to avoid audiotransients. The broadcaster can hear a live monitor mix throughheadphones, with an adjustable level for monitoring their localmicrophone.The application is aware of when prerecorded audio is playing in themix, and each audio track's descriptive metadata is shown to theaudience in synchronization with what they are hearing.The guest interface supports a single live audio source from a choiceof any local microphone.The audience interface delivers the channel's broadcast mix, but alsooffers basic volume and EQ controlplus the ability to pause/rewind/resume the live stream. Optionally,the listener can slow down the content of the audio without changingits pitch, for example to aid in understanding a foreign language.An advanced feature would give the audience control over the mixitself. The mix of tracks and sources created by the broadcaster wouldbe a default, but the listener would have the ability to create adifferent mix. For instance, in the case of a radio play with a mix ofvoices, sound effects and music, the listener could be offered aninterface to control the relative volume of the voices to effects andmusic, or create a binaural mix tailored specifically to their taste.Such a feature would provide valuable personalization of the radioexperience, as well as significant accessibility enhancements.Notes andImplementation ConsiderationsAs with the Video Chat Application scenario, streaming and localdevice discovery and access within this scenario are handled bythe WebReal-Time Communication API. The local audio processing inthis scenario highlights the requirement that RTC streams andWeb Audio be tightly integrated. Incoming MediaStreams mustbe able to be exposed as audio sources, and audio destinationsmust be able to yield an outgoing RTC stream. For example, thebroadcaster's browser employs a set of incoming MediaStreams frommicrophones, remote participants, etc., locally processes theiraudio through a graph of AudioNodes, and directs theoutput to an outgoing MediaStream representing the live mix forthe show.Building this application requires the application of gaincontrol, panning, audio effects and blendingof multiple mono and stereo audio sources to yield astereo mix. Some relevant features in the API include AudioGainNode,ConvolverNode, AudioPannerNode.Noise gating (suppressing output when a source's levelfalls below some minimum threshold) is highly desirable formicrophone inputs to avoid stray room noise being included in thebroadcast mix. This could be implemented as a custom algorithmusing a JavaScriptAudioNode.To drive the visual feedback to the broadcaster on audio sourceactivity and to control automatic ducking, this scenario needs away to easily detect the time-averaged signal level on agiven audio source. The Web Audio API does not currently provide aprepackaged way to do this, but it can be implemented with customJS processing or an ultra-low-pass filter built with BiquadFilterNode.Ducking affects the level of multiple audio sources at once,which implies the ability to associate a single dynamic audioparameter to the gain associated with these sources' signalpaths. The specification's AudioGain interfaceprovides this.Smooth muting requires the ability to smoothly automate gainchanges over a time interval, without usingbrowser-unfriendly coding techniques like tight loops orhigh-frequency callbacks. The parameter automationfeatures associated with AudioParam are useful forthis kind of feature.Pausing and resuming the show on the audience side implies theability to buffer data received from audio sources inthe processing graph, and also to send buffered data to audiodestinations.Speed changes are currently unsupported by the Web Audio API.Thus, the functionality for audio speed changing, a customalgorithm, requires the ability to create custom audiotransformations using a browser programming language (e.g.JavaScriptAudioNode). When audio delivery is sloweddown, audio samples will have to be locally buffered by theapplication up to some allowed limit, since they continue to bedelivered by the incoming stream at a normal rate.There is a standard way to access a set of metadataproperties for media resources with the following W3Cdocuments:Ontologyfor Media Resources 1.0. This document defines a coreset of metadata properties for media resources, along withtheir mappings to elements from a set of existing metadataformats.APIfor Media Resources 1.0. This API provides developerswith a convenient access to metadata information stored indifferent metadata formats. It provides means to access theset of metadata properties defined in the Ontology for MediaResources 1.0 specification.The ability for the listeners to create their own mix rely on thepossibility of sending multiple tracks in the RTC stream. This is inscope of the current WebRTC specification, where one MediaStreamcan have multiple MediaStreamTracks.2.5 Music Creation Environment withSampled InstrumentsA composer is employing a web-based application to create and edit amusical composition with live synthesized playback. The user interfacefor composing can take a number of forms including conventionalWestern notation and a piano-roll style display. The document can besonically rendered on demand as a piece of music, i.e. aseries of precisely timed, pitched and modulated audio events (notes).The musician occasionally stops editing and wishes to hear playbackof some or all of the score they are working on to take stock of theirwork. At this point the program performs sequenced playback of someportion of the document. Some simple effects such as instrumentpanning and room reverb are also applied for a more realistic andsatisfying effect.Compositions in this editor employ a set of instrument samples, i.e.a pre-existing library of recorded audio snippets. Any given snippetis a brief audio recording of a note played on an instrument with somespecific and known combination of pitch, dynamics and articulation.The combinations in the library are necessarily limited in number toavoid bandwidth and storage overhead. During playback, the editor mustsimulate the sound of each instrument playing its part in thecomposition. This is done by transforming the available pre-recordedsamples from their original pitch, duration and volume to match thecharacteristics prescribed by each note in the composed music. Theseper-note transformations must also be scheduled to be played at thetimes prescribed by the composition.During playback a moving cursor indicates the exact point in themusic that is being heard at each moment.At some point the user exports an MP3 or WAV file from the programfor some other purpose. This file contains the same audio rendition ofthe score that is played interactively when the user requested itearlier.Notes andImplementation ConsiderationsInstrument samples must be able to be loaded into memory forfast processing during music rendering. These pre-loaded audiosnippets must have a one-to-many relationship with objects in theWeb Audio API representing specific notes, to avoid duplicatingthe same sample in memory for each note in a composition that isrendered with it. The API's AudioBuffer and AudioBufferSourceNodeinterfaces address this requirement.It must be possible to schedule large numbers of individualevents over a long period of time, each of which is atransformation of some original audio sample, without degradingreal-time browser performance. A graph-based approach such as thatin the Web Audio API makes the construction of any giventransformation practical, by supporting simple recipes forcreating sub-graphs built around a sample's pre-loaded AudioBuffer.These subgraphs can be constructed and scheduled to be played inthe future. In one approach to supporting longer compositions, theconstruction and scheduling of future events can be kept "toppedup" via periodic timer callbacks, to avoid the overhead ofcreating huge graphs all at once.A given sample must be able to be arbitrarily transformed inpitch and volume to match a note in the music. AudioBufferSourceNode'splaybackRate attribute provides the pitch-changecapability, while AudioGainNode allows the volume tobe adjusted.A given sample must be able to be arbitrarily transformed induration (without changing its pitch) to match a note in themusic. AudioBufferSourceNode's looping parametersprovide sample-accurate start and end loop points, allowing a noteof arbitrary duration to be generated even though the originalrecording may be brief.Looped samples by definition do not have a clean ending. To avoidan abrupt glitchy cutoff at the end of a note, a gain and/orfilter envelope must be applied. Such envelopes normally follow anexponential trajectory during key time intervals in the life cycleof a note. The AudioParam features of the Web AudioAPI in conjunction with AudioGainNode and BiquadFilterNodesupport this requirement.It is necessary to coordinate visual display with sequencedplayback of the document, such as a moving cursor or highlightingeffect applied to notes. This implies the need to programmaticallydetermine the exact time offset within the performance of thesound being currently rendered through the computer's audio outputchannel. This time offset must, in turn, have a well-definedrelationship to time offsets in prior API requests to schedulevarious notes at various times. The API provides such a capabilityin the AudioContext.currentTime attribute.To export an audio file, the audio rendering pipeline must beable to yield buffers of sample frames directly, rather than beingforced to an audio device destination. Built-in codecs totranslate these buffers to standard audio file output formats arealso desirable.Typical per-channel effects such as stereo pan control must bereadily available. Panning allows the sound output for eachinstrument channel to appear to occupy a different spatiallocation in the output mix, adding greatly to the realism of theplayback. Adding and configuring one of the Web Audio API's AudioPannerNodefor each channel output path provides this capability.Typical master bus effects such as room reverb must be readilyavailable. Such effects are applied to the entire mix as a finalprocessing stage. A single ConvolverNode is capableof simulating a wide range of room acoustics.2.6 Connected DJboothA popular DJ is playing a live set,using a popular web-based DJsoftware. The web application allows her to perform both in the clubwhere she is mixing, as well as online, with tens of thousands joininglive to enjoy the set.The DJ-deck web interface offersthe typical features of decks and turntables. While a first track isplaying and its sound sent to both the sound system in the club andstreamed to the web browsers of fans around the world, the DJwould be able to quickly select several other track, play them throughheadphones without affecting the main audio output of the application,and match them to the track currently playing through a mix ofpausing, skipping forward or back and pitch/speed change. Theapplication helps automate a lot of this work: by measuring the beatof the current track at 125BPM and the one of the chosen next track at140 BPM, it can automaticallyslow down the second track, and even position it to match the beats ofthe one currently playing.Once the correct match is reached, The DJwould be able to start playing the track in the main audio output,either immediately or by slowly changing the volume controls for eachtrack. She uses a cross fader to let the new song blend into the oldone, and eventually goes completely across so only the new song isplaying. This gives the illusion that the song never ended.At the other end, fans listening to the set would be able to watch avideo of the DJ mixing, accompaniedby a graphic visualization of the music, picked from a variety ofchoices: spectrum analysis, level-meter view or a number of 2D or 3Dabstract visualizations displayed either next to or overlaid on thetitle="Disc Jockey">DJ video.Notes andImplementation ConsiderationsAs in many other scenarios in this document, it is expected thatAPIs such as the WebReal-Time Communication API will be used for the streaming ofaudio and video across a number of clients.One of the specific requirements illustrated by this scenario isthe ability to have two different outputs for the sound: one forthe headphones, and one for the music stream sent to all theclients. With the typical web-friendly hardware, this would bedifficult or impossible to implement by considering both as audiodestinations, since they seldom have or allow two sound outputs tobe used at the same time. And indeed, in the current Web Audio APIdraft, a given AudioContext can only use one AudioDestinationNodeas destination.However, if we consider that the headphones are the audio output,and that the streaming DJ set isnot a typical audio destination but an outgoing MediaStreampassed on to the WebRTC API, it should be possible to implementthis scenario, sending output to both headphones and the streamand gradually sending sound from one to the other withoutaffecting the exact state of playback and processing of a source.With the Web Audio API, this can be achieved by using the createMediaStreamDestination()interface.This scenario makes heavy usage of audio analysis capabilities,both for automation purposes (beat detection and beat matching) andvisualization (spectrum, level and other abstract visualizationmodes).The requirement for pitch/speed change are not currently coveredby the Web Audio API's native processing nodes. Such processingwould probably have to be handled with custom processing nodes.2.7 Playful sonification of userinterfacesA child is visiting a social website designed for kids. The playful,colorful HTML interface is accompanied by sound effects played as thechild hovers or clicks on some of the elements of the page. Forexample, when filling in a form the sound of a typewriter can be heardas the child types in the form field. Some of the sounds arespatialized and have a different volume depending on where and how thechild interacts with the page. When an action triggers a downloadvisualized with a progress bar, a gradually rising pitch soundaccompanies the download and another sound (ping!) is played when thedownload is complete.Notes andImplementation ConsiderationsAlthough the web UIincorporates many sound effects, its controls are embedded in thesite's pages using standard web technology such as HTML formelements and CSS stylesheets. JavaScript event handlers may beattached to these elements, causing graphs of AudioNodesto be constructed and activated to produce sound output.Modularity, spatialization and mixing play an important role inthis scenario, as for the others in this document.Various effects can be achieved through programmatic variation ofthese sounds using the Web Audio API. The download progress couldsmoothly vary the pitch of an AudioBufferSourceNode'splaybackRate using an exponential ramp function, or amore realistic typewriter sound could be achieved by varying anoutput filter's frequency based on the keypress's character code.In a future version of CSS, stylesheets may be able to supportsimple types of sonification, such as attaching a "typewriter key"sound to an HTML textarea element or a "click" soundto an HTML button. These can be thought of as anextension of the visual skinning concepts already embodied bystyle attributes such as background-image.2.8 Podcast on a flightA traveler is subscribed to a podcast, and has previously downloadedan audio book on his device using the podcast's web-based application.The audio files are stored locally on his device, giving simple andconvenient access to episodic content whenever the user wishes tolisten.Sitting in an airplane for a 2-hour flight, he opens the podcastapplication in his HTML browser and sees that the episode he hasselected lasts 3 hours. The application offers a speed-up feature thatallows the speech to be delivered at a faster than normal speedwithout pitch distortion ("chipmunk voices"). He sets the auditiontime to 2 hours in order to finish the audio book before landing. Healso sets the sound control in the application to "Noisy Environment",causing the sound to be equalized for greatest intelligibility in anoisy setting such as an airplane.Notes andImplementation ConsiderationsLocal audio can be downloaded, stored and retrieved using thehref="http://www.w3.org/TR/FileAPI/">HTML File API.This scenario requires a special audio transformation that cancompress the duration of speech without affecting overall timbreand intelligibility. In the Web Audio API this function isn'tnatively supported but could be accomplished through attachingcustom processing code to a JavaScriptAudioNode.The "Noisy Environment" setting could be accomplished throughequalization features in the Web Audio API such as BiquadFilterNodeor ConvolverNode.2.9 Short film with director'scommentary and audio descriptionA video editor is using an online editing tool to refine thesoundtrack of a short film. Once the video is ready, she will workwith the production team to prepare an audio description of the scenesto make the video work more accessible to people with sightimpairments. The video director is also planning to add an audiocommentary track to explain the creative process behind the film.Using the online tool, the video editor extracts the existingrecorded vocals from the video stream, modifies their levels andperforms other modifications of the audio stream. She also addsseveral songs, including a orchestral background and pop songs, atdifferent parts of the film soundtrack. Several Foley effects(footsteps, doors opening and closing, etc.) are also added to makethe soundscape of each scene complete.While editing, the online tool must ensure that the audio and videoplayback are synchronized, allowing the editor to insert audio samplesat the right time. As the length of one of the songs is slightlydifferent from the video segment she is matching it with, she cansynchronize the two by slightly speeding up or slowing down the audiotrack. The final soundtrack is mixed down into the final soundtrack,added to the video as a replacement for the original audio track, andsynced with the video track.Once the audio description and commentary are recorded, the film,displayed in a HTML web page, can be played with its original audiotrack (embedded in the video container) or with any of the audiocommentary tracks loaded from a different source and synchronized withthe video playback. When there's audio on the commentary track, themain track volume is reduced (ducked) gradually and smoothly broughtback to full volume when the commentary / description track is silent.The visitor can switch between audio tracks on the fly, withoutaffecting the video playback. Pausing the video playback also pausesthe commentary track, which then remains in sync when playbackresumes.Notes andImplementation ConsiderationsThis scenario is, in many ways, fairly similar to a number ofothers already discussed throughout the document. The ability to layout a number of sources and mix them in a consistent soundtrack isthe subject of the Onlinemusic production tool scenario, while some effects such asducking have already been discussed in the Onlineradio broadcast scenario.Essentially, this use case illustrates the need to do all thesethings in sync with video. In the context of the open web platform,it means that audio processing API to integrate with the HTML5MediaController interface.2.10 Web-based guitar practice serviceA serious guitar player uses a web-based tool to practice a new tune.Connecting a USB microphone and a pair of headphones to theircomputer, the guitarist is able to tune an acoustic guitar using agraphical interface and set a metronome for the practice session. Amix of one or more backing tracks can be optionally selected for theguitarist to play along with, with or without the metronome present.During a practice session, the microphone audio is analyzed todetermine whether the guitarist is playing the correct notes in tempo,and visual feedback is provided via a graphical interface of guitartablature sheet music with superimposed highlighting.The guitarist's performance during each session is recorded,optionally mixed with the audio backing-track mix. At the conclusionof a session, this performance can be saved to various file formats oruploaded to an online social music service for sharing and commentarywith other users.Notes andImplementation ConsiderationsThe audio input reflects the guitarist's performance, which isitself aurally synchronized by the guitarist to the current audiooutput. The scenario requires that the input be analyzed forcorrect rhythmic and pitch content. Such an algorithm can beimplemented in a JavaScriptAudioNode.Analysis of the performance in turn requires measurement of thereal-time latency in both audio input and output, so that thealgorithm analyzing the live performance can know the temporalrelationship of a given output sample (reflecting the metronomeand/or backing track) to a given input sample (reflecting theguitarist playing along with that output). These latencies areunpredictable from one system to another and cannot be hard-coded.Currently the Web Audio API lacks such support.This scenario uses a mixture of sound sources including a livemicrophone input, a synthesized metronome and a set ofpre-recorded audio backing tracks (which are synced to a fixedtempo). The mixing of these sources to the browser's audio outputcan be accomplished by a combination of instances of AudioGainNodeand AudioPannerNode.The live input requires microphone access, which it isanticipated will be available viatitle="HTML Media Capture">HTML Media Capture bridgedthrough an AudioNode interface.Pre-recorded backing tracks can be loaded into AudioBuffersand used as sample-accurate synced sources by wrapping these in AudioBufferSourceNodeinstances.Metronome synthesis can be accomplished with a variety of meansprovided by the Web Audio API. In one approach, an implementercould use an Oscillator square-wave source togenerate the metronome sound. A timer callback repeatedly runs ata low frequency to maintain a pool of these instances scheduled tooccur on future beats in the music (which can be sample-accuratelysynced to offsets in the backing tracks given the lock-step timingin the Web Audio API).Programmatic output of a recorded session's audio buffer must beaccomplished to files (via the HTML5 File API) or upload streams(via MediaStreams or HTTP). The scenario implies the use of one ormore encoders on this buffered data to yield the supported audiofile formats. Native audio-to-file encoding is not currentlysupported by the Web Audio API and thus would need to beimplemented in JavaScript.2.11 User Control of AudioA programmer wants to create a browser extension to allow the user tocontrol the volume of audio.The extension should let the user control the audio volume on aper-tab basis, or to kill any audio playing completely. The extensiondeveloper wishes to make sure killing the audio is done in a way thattakes care of garbage collection.Among the features sometimes requested for his extension are theability to limit the audio volume to an acceptable level, both per taband globally. On operating systems that allow it, the developer wouldalso like his extension to mute or pause sound when a critical systemsound is being played.Notes andImplementation ConsiderationsThis function is likely to combine usage of both abrowser-specific extension API and the Web Audio API. One way toimplement this scenario would be to use a browser-dependent API toiterate through a list of window objects, and then for each windowobject iterate through a list of active AudioContexts and managetheir volume (or, more conveniently, manage some kind of masteraudio volume for the window). Neither of these latter approachesare currently supported by the Web Audio API.The ability to mute or pause sounds when the Operating Systemfires a critical system sound is modelled after the feature inexisting Operating Systems which will automatically muteapplications when outputting a system sound. As such, this may notinvolve any specific requirement for the Web Audio API. However,because some operating systems may implement such a feature, WebAudio apps may want to be notified of the muting and actaccordingly (suspend, pause, etc). There may therefore be arequirement for the Web Audio API to provide such an eventhandler.A. AcknowledgementsThis document is the result of the work of the W3CAudio Working Group. Membersof the working group, at the time of publication, included:Bateman, Adrian (Microsoft Corporation);Berkovitz, Joe (Invited expert);Cardoso, Gabriel (INRIA);Carlson, Eric (Apple, Inc.);Chen, Bin (Baidu, Inc.);Geelnard, Marcus (Opera Software);Goode, Adam (Google, Inc.);Gregan, Matthew (Mozilla Foundation);JÃƒÂ¤genstedt, Philip (Opera Software);Kalliokoski, Jussi (Invited expert);Lowis, Chris (British Broadcasting Corporation);MacDonald, Alistair (Invited Expert);Mandyam, Giridhar (Qualcomm Innovation Center, Inc);Michel, Thierry (W3C/title="European Research Consortium for Informatics and Mathematics">ERCIM);Noble, Jer (Apple, Inc.);O'Callahan, Robert (Mozilla Foundation);Olivier, Frank (Microsoft Corporation);Paradis, Matthew (British Broadcasting Corporation);Peraza Barreras, Jorge Armando (Microsoft Corporation);Raman, T.V. (Google, Inc.);Rogers, Chris (Google, Inc.);Schepers, Doug (W3C/title="Massachusetts Institute of Technology">MIT);Shires, Glen (Google, Inc.);Smith, Michael (W3C/Keio);Thereaux, Olivier (British Broadcasting Corporation);Wei, James (Intel Corporation);Wilson, Chris (Google,Inc.);Young, Milan (Nuance Communications, Inc.).The people who have contributed to discussionson public-audio@w3.org are also gratefully acknowledged.This document was also heavily influenced by earlier work by the audioworking group and others, including:A list of "title="Audio API Use Cases - Audio Incubator">Core Use Cases"authored by thetitle="World Wide Web Consortium">W3C Audio Incubator Group,which predated the W3CAudio Working GroupThetitle="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2">usecases requirements from Web RTCThetitle="http://www.w3.org/TR/2011/WD-streamproc-20111215/#scenarios">Scenariosfrom the Media Streams Processing